{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "366edae6-c36d-4b71-9d27-83de8f4d7ee7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Extremwertbestimmung f√ºr Funktionen von mehreren Ver√§nderlichen mit dem Gradientenverfahren\n",
    "\n",
    "## Anselm Hudde\n",
    "\n",
    "![](optimization.png)\n",
    "**Quelle: Venter, G. (2010). Review of optimization techniques.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bda742a-3bc3-4d75-9c5d-867099ff875c",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run Gradientenverfahren.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e78631",
   "metadata": {},
   "source": [
    "## Minimierung von Funktionen mehrerer Ver√§nderlicher\n",
    "\n",
    "In der Praxis ist es meistens nicht m√∂glich, einen algebraischen Ausdruck f√ºr das Minimum einer Funktion\n",
    "$$\n",
    "f \\colon \\mathbb R^n \\to \\mathbb R\n",
    "$$\n",
    "zu bestimmen.\n",
    "Es wird hier mit **numerischen Methoden** gearbeitet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bb3a6e-f567-4a67-8a00-e22646a019bb",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "## Beispielfunktion\n",
    "\n",
    "Wir betrachten die Funktion\n",
    "\n",
    "\\begin{equation}f(x_{1}, x_{2}) = \\sin(x_{1}) + 0.05 x_{1}^2 + 0.1 x_{2}^2, \\end{equation}\n",
    "\n",
    "welche wir minimieren wollen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be85e606",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    return np.sin(x[0]) + 0.05*x[0]**2 + 0.1*x[1]**2\n",
    "\n",
    "surface_plot = plot()\n",
    "surface_plot.plot_surface(-5,1,-3,3,f)\n",
    "\n",
    "contour_plot = plot()\n",
    "contour_plot.plot_contour(-5,1,-3,3,f)\n",
    "\n",
    "show_plot(contour_plot, surface_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e406e17b-70d2-4f3f-81b2-3354f42acf0e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Gradientabstieg: Die Idee\n",
    "\n",
    "Die allgemeine Idee des Gradientenabstiegs ist es, immer in die Richtung des steilsten Abstiegs zu gehen.\n",
    "\n",
    "Hierzu wiederholen wir den Gradienten: \n",
    "\n",
    "**Definition:**\n",
    "Der Gradient der Funktion $f \\colon \\mathbb{R}^{n} \\to \\mathbb{R}$ ist die Funktion\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla f \\colon \\mathbb{R}^{n} \\to \\mathbb{R}^{n};~~\n",
    "x =\n",
    "\\begin{pmatrix}\n",
    "x_{1} \\\\ \\vdots \\\\ x_{n}\n",
    "\\end{pmatrix}\n",
    "\\mapsto\n",
    "\\begin{pmatrix}\n",
    "\\tfrac{\\partial f}{\\partial x_{1}} (x) \\\\ \\vdots \\\\ \\tfrac{\\partial f}{\\partial x_{n}} (x)\n",
    "\\end{pmatrix}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cf3abe-4911-45c0-99b7-78eb125d5cce",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Intuition:** Der Gradient zeigt immer in die Richtung des steilsten Aufstiegs.\n",
    "\n",
    "**Beispiel 1:** $f(x_{1}, x_{2}) = x_1$, $\\nabla f(x_{1}, x_{2}) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7392913a-0e0c-4456-8938-893f47d3cd1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "surface_plot = plot()\n",
    "surface_plot.plot_surface(0,4,0,4,function = lambda x: x[0])\n",
    "surface_plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13632e34-1665-4598-8873-2bc804728365",
   "metadata": {},
   "source": [
    "**Beispiel 2:** $f(x_{1}, x_{2}) = \\tfrac{1}{2} (x_{1} + x_{2})$, $\\nabla f(x_{1}, x_{2}) = \\begin{pmatrix} \\tfrac{1}{2} \\\\ \\tfrac{1}{2} \\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cac26d-b0e0-45c8-aa7e-34766f499044",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "surface_plot = plot()\n",
    "surface_plot.plot_surface(0,4,0,4,function = lambda x: (1/3)*(x[0]+x[1]))\n",
    "surface_plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3635e17b-370a-41e8-9254-f9d3139664f6",
   "metadata": {},
   "source": [
    "## Berechnung des Gradienten von $f$\n",
    "\n",
    "Der Gradient von \n",
    "$ùëì(x_{1}, x_{2})= \\sin(ùë•_{1})+0.05 ùë•_{1}^{2} + 0.1 x_{2}^{2}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla f(x_{1}, x_{2})\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial f}{\\partial x_{1}} \\\\ \\frac{\\partial f}{\\partial x_{2}} \\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "\\cos(x_{1}) + 0.1 x_{1} \\\\ 0.2 x_{2}\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "**Beispiele:**\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla f(-4, -2) =\n",
    "\\begin{pmatrix}\n",
    "-1.05 \\\\ -0.40\n",
    "\\end{pmatrix},\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\nabla f(-1, 2) =\n",
    "\\begin{pmatrix}\n",
    "0.44 \\\\ 0.20\n",
    "\\end{pmatrix}.\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82db4c94-24fe-4804-90df-351454a787df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def grad_f(x):\n",
    "    return np.array([np.cos(x[0])+0.1*x[0], 0.2*x[1]])\n",
    "\n",
    "contour_plot.add_gradients(grad_f)\n",
    "contour_plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5117f6f",
   "metadata": {},
   "source": [
    "## Die Schritte des Gradientenverfahrens\n",
    "\n",
    "Wir starten in einem Punkt $x^{(0)} \\in \\mathbb{R}^{2}$.\n",
    "F√ºr den $n$-ten Schritt berechnen wir den Gradienten\n",
    "$\\nabla f(x^{(n)})$,\n",
    "und gehen einen Schritt in die Gegenrichtung, wobei wir den Gradienten vorher mit der Lernrate $\\gamma$ multiplizieren:\n",
    "\n",
    "\\begin{equation}\n",
    "\tx^{(n+1)}= x^{(n)} - \\gamma \\nabla f({x}^{(n)}),\\ n\\geq 0.\n",
    "\\end{equation}\n",
    "\n",
    "### Lernrate $\\gamma = 1$:\n",
    "\n",
    "Wir starten im Punkt\n",
    "$x^{(0)} = \\begin{pmatrix} -4 \\\\ -2 \\end{pmatrix}$\n",
    "und mit einer Lernrate von $\\gamma = 1$.\n",
    "Dann haben wir\n",
    "\n",
    "\\begin{equation} \\nabla f(x^{(0)}) =\n",
    "\\begin{pmatrix}\\cos(-4)+0.1\\cdot(-4) \\\\ 0.2\\cdot(-2) \\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix} -1.05 \\\\ -0.40 \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "und damit\n",
    "\n",
    "\\begin{equation}\n",
    "x^{(1)} = x^{(0)} + \\gamma_{0} \\cdot \\nabla f(x^{(0)})\n",
    "=\n",
    "\\begin{pmatrix} -4 \\\\ -2 \\end{pmatrix}\n",
    "- 1 \\cdot \n",
    "\\begin{pmatrix} -1.05 \\\\ -0.40 \\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "-2.95 \\\\ -1.60\n",
    "\\end{pmatrix}.\n",
    "\\end{equation}\n",
    "Die n√§chsten Schritte folgen analog:\n",
    "\n",
    "$x^{(2)} = \\begin{pmatrix} -1.67 \\\\ -1.28\\end{pmatrix}, ~\n",
    "x^{(3)} = \\begin{pmatrix} -1.40 \\\\ -1.02\\end{pmatrix}, ~\\dots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea11cba-4570-4aaf-b9cf-6e1f4b898999",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Gradientenabstieg mit Lernrate gamma = 1:\n",
    "\n",
    "def f(x):\n",
    "    return np.sin(x[0]) + 0.05*x[0]**2 + 0.1*x[1]**2\n",
    "\n",
    "def grad_f(x):\n",
    "    return np.array([np.cos(x[0])+0.1*x[0], 0.2*x[1]])\n",
    "\n",
    "contour_plot = plot()\n",
    "contour_plot.plot_contour(-5,1,-3,3,f)\n",
    "\n",
    "surface_plot = plot()\n",
    "surface_plot.plot_surface(-5,1,-3,3,f, opacity=0.5)\n",
    "\n",
    "i = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041ea80e",
   "metadata": {},
   "source": [
    "Die folgende Zelle f√ºhrt die einzelnen Schritte des Gradientenverfahrens mit der Lernrate $\\gamma = 1$ aus.\n",
    "Die wiederholte Ausf√ºhrung dieser Zelle zeigt, wie das Gradientenverfahren konvergiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5617af",
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_plot.add_gradient_descent(x0=[-4, -2], function = f, grad=grad_f, gamma=1, iterations=i, color = \"#636EFA\")\n",
    "surface_plot.add_gradient_descent_surface(x0=[-4, -2], function = f, grad=grad_f, gamma=1, iterations=i, color = \"#636EFA\")\n",
    "show_plot(contour_plot, surface_plot)\n",
    "i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d308b5e-5660-44a2-aabe-cf2e385b9d8a",
   "metadata": {},
   "source": [
    "### Das Gradientenverfahren mit einer Lernrate von $\\gamma = 0.1$ ist langsamer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031e363b-6d69-4bd9-84a6-9ca6a5d5fed8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d16681b",
   "metadata": {},
   "source": [
    "Die folgenden Zelle f√ºhrt die einzelnen Schritte des Gradientenverfahrens mit der Lernrate $\\gamma = 0.1$ durch.\n",
    "Wenn wir diese Zelle mehrmals hintereinander ausf√ºhren sehen wir, wie das Gradientenverfahren langsam konvergiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669954b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradientenabstieg mit Lernrate gamma = 0.1:\n",
    "\n",
    "contour_plot.add_gradient_descent(x0=[-4, -2], function = f, grad=grad_f, gamma=0.1, iterations=i, color = \"#EF553B\")\n",
    "surface_plot.add_gradient_descent_surface(x0=[-4, -2], function = f, grad=grad_f, gamma=0.1, iterations=i, color = \"#EF553B\")\n",
    "show_plot(contour_plot, surface_plot)\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba80109",
   "metadata": {},
   "source": [
    "### Gradientenverfahren mit Lernrate $\\gamma = 2$ konvergiert nicht:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bd4ddd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa58aa31",
   "metadata": {},
   "source": [
    "Die folgenden Zelle f√ºhrt die einzelnen Schritte des Gradientenverfahrens mit der Lernrate $\\gamma = 2$ durch.\n",
    "Wenn wir diese Zelle mehrmals hintereinander ausf√ºhren sehen wir, dass das Gradientenverfahren in diesem Fall nicht konvergiert.\n",
    "Die L√∂sung springt hin- und her."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62039c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradientenabstieg mit Lernrate gamma = 2:\n",
    "\n",
    "contour_plot.add_gradient_descent(x0=[-4, -2], function = f, grad=grad_f, gamma=2, iterations=i, color = \"#00CC96\")\n",
    "surface_plot.add_gradient_descent_surface(x0=[-4, -2], function = f, grad=grad_f, gamma=2, iterations=i, color = \"#00CC96\")\n",
    "show_plot(contour_plot, surface_plot)\n",
    "i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48f4425-5044-4b9e-81f2-70385c1a0eca",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "## Einfluss des Startpunktes:\n",
    "\n",
    "Wenn die Funktion $f$ mehrere lokale Minima hat, spielt die Wahl des Startpunktes eine gro√üe Rolle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ff7250-8aed-4c9b-a5ee-1447587bbd65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def f(x):\n",
    "    return np.sin(x[0]) + 0.05*x[0]**2 + 0.1*x[1]**2\n",
    "\n",
    "contour_plot = plot()\n",
    "contour_plot.plot_contour(-5,6,-3,3,f)\n",
    "\n",
    "surface_plot = plot()\n",
    "surface_plot.plot_surface(-5,6,-3,3,f)\n",
    "\n",
    "show_plot(contour_plot, surface_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5647d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "x0=[random.uniform(-5,6),random.uniform(-3,3)]\n",
    "\n",
    "contour_plot.add_gradient_descent(x0=x0, function = f, grad=grad_f, gamma=1, iterations=30)\n",
    "surface_plot.add_gradient_descent_surface(x0=x0, function = f, grad=grad_f, gamma=1, iterations=30)\n",
    "show_plot(contour_plot, surface_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39148f36-835e-4ef2-9073-5764c20f03f1",
   "metadata": {},
   "source": [
    "## Das Gradientenverfahren mit `SciPy`\n",
    "\n",
    "Die vorgestellte Methode ist f√ºr den praktischen Einsatz noch zu einfach, in der praktischen Anwendung gibt es noch viele Probleme zu beachten.\n",
    "Daher nimmt man in der Regel eine bereits vorhandene Implementierung.\n",
    "Das Python-Paket `SciPy` mit der Funktion `minimize` ist daf√ºr sehr gut geeignet.\n",
    "Es gen√ºgt, die zu minimierende Funktion und den Startpunkt einzugeben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1739a455",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def f(x):\n",
    "    return np.sin(x[0]) + 0.05*x[0]**2 + 0.1*x[1]**2\n",
    "\n",
    "minimize(fun=f, x0=np.array([-4,-2]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "92f23b4ac9a1ac0aabac2a5fe090cf874f268b01de51ad4092840717b54c19ba"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
